{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Learning Rates\n",
    "\n",
    "The learning rate is one of the most important hyperparameters when training a neural network. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges - that is, where it's performance begins to suffer). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., $10^{–5}$) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., to go from $10^{–5}$ to 10 in 500 iterations). \n",
    "\n",
    "If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate.  Here's how to do this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.1.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:55:16.386323: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-09 15:55:17.773090: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-09 15:55:21.101516: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 15:55:25.165362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Using Fashion MNIST as a working example\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "def build_model(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "def build_and_train_model(optimizer):\n",
    "    model = build_model()\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model.fit(X_train, y_train, epochs=10,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:  Increasing the learning rate**\n",
    "\n",
    "The following callback increases the learning rate by a factor in each batch, and also keeps track of the losses.  This is a little kludgey because keras only reports mean loss (over all batches thus far) so we need to keep track of things to figure out how much loss is incurred by each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "K = tf.keras.backend\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.sum_of_epoch_losses = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        mean_epoch_loss = logs[\"loss\"]  # the epoch's mean loss so far \n",
    "        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)\n",
    "        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses\n",
    "        self.sum_of_epoch_losses = new_sum_of_epoch_losses\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.learning_rate,\n",
    "                    self.model.optimizer.learning_rate * self.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Probing the model**\n",
    "\n",
    "The following routine trains a model using the above callback to update the learning rate through a range of values, and then resets the model when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
    "                       max_rate=1):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Inspecting the data**\n",
    "\n",
    "Finally, we simply plot the losses against the learning rate to identify a \"peak\" and then we'll back off from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses, \"b\")\n",
    "    plt.gca().set_xscale('log')\n",
    "    max_loss = losses[0] + min(losses)\n",
    "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
    "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the model and find the optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m      4\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 6\u001b[0m rates, losses \u001b[38;5;241m=\u001b[39m \u001b[43mfind_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plot_lr_vs_loss(rates, losses)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mfind_learning_rate\u001b[0;34m(model, X, y, epochs, batch_size, min_rate, max_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m factor \u001b[38;5;241m=\u001b[39m (max_rate \u001b[38;5;241m/\u001b[39m min_rate) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m iterations)\n\u001b[1;32m      6\u001b[0m init_lr \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mget_value(model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m exp_lr \u001b[38;5;241m=\u001b[39m ExponentialLearningRate(factor)\n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     10\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[exp_lr])\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/legacy/backend.py:1883\u001b[0m, in \u001b[0;36mset_value\u001b[0;34m(x, value)\u001b[0m\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.set_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_value\u001b[39m(x, value):\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1883\u001b[0m     value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m)\n\u001b[1;32m   1884\u001b[0m     x\u001b[38;5;241m.\u001b[39massign(value)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train, y_train, epochs=1,\n",
    "                                   batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests an optimal learning rate around $ 10^{-1} $.  So, let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.5559 - accuracy: 0.7956 - val_loss: 0.4409 - val_accuracy: 0.8330\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4018 - accuracy: 0.8510 - val_loss: 0.4458 - val_accuracy: 0.8244\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3602 - accuracy: 0.8666 - val_loss: 0.3932 - val_accuracy: 0.8564\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3365 - accuracy: 0.8761 - val_loss: 0.3846 - val_accuracy: 0.8638\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3182 - accuracy: 0.8813 - val_loss: 0.3397 - val_accuracy: 0.8740\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3018 - accuracy: 0.8871 - val_loss: 0.3584 - val_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2910 - accuracy: 0.8903 - val_loss: 0.3351 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2789 - accuracy: 0.8950 - val_loss: 0.3689 - val_accuracy: 0.8664\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2700 - accuracy: 0.8999 - val_loss: 0.3441 - val_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2598 - accuracy: 0.9010 - val_loss: 0.3334 - val_accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "optimal_lr = 0.1\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=optimal_lr)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "history_optimal_lr = model.fit(X_train, y_train, epochs=10,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Verify that 0.1 is a better learning rate than something much smaller (e.g. .001) or much larger (e.g., .5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules\n",
    "\n",
    "A constant learning rate doesn't account for the possibility that the gradient landscape changes as we move across it.  Optimizers can help address this, but it is also possible to adjust the learning rate according to a schedule.  In the following, we discuss a few approaches for this.\n",
    "\n",
    "#### Motivation?\n",
    "\n",
    "1. **Early Training:** Initially, a larger learning rate can be beneficial, enabling rapid learning and helping the model to quickly escape suboptimal local minima.\n",
    "2. **Later Stages:** As training progresses, reducing the learning rate helps to stabilize the learning process and fine-tune the model parameters, leading to better convergence.\n",
    "3. **Preventing Overfitting and Oscillations:** A carefully chosen learning rate schedule can prevent the model from overfitting and reduce oscillations near the minima.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "\n",
    "![learning rate](assets/learning_rate.png)\n",
    "</div>\n",
    "\n",
    "Learning rate scheduling is thus a balancing act - starting with a higher learning rate for faster convergence initially, then decreasing it to allow more fine-grained adjustments as the model starts converging.\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **Power Scheduling:**\n",
    "   - **Description:** Gradually decreases the learning rate over time using a polynomial decay, typically quadratic.\n",
    "   - **Formulation:** $ \\eta(t) = \\eta_0 / (1 + t/s)^c $, where $ \\eta_0 $ is the initial learning rate, $ t $ is the iteration number, $ s $ is a step parameter, and $ c $ is the power, often set to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer name SGD\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5278 - accuracy: 0.8069 - val_loss: 0.4337 - val_accuracy: 0.8364\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3729 - accuracy: 0.8636 - val_loss: 0.3861 - val_accuracy: 0.8478\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3363 - accuracy: 0.8757 - val_loss: 0.3594 - val_accuracy: 0.8698\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3185 - accuracy: 0.8836 - val_loss: 0.3381 - val_accuracy: 0.8756\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3048 - accuracy: 0.8882 - val_loss: 0.3331 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2944 - accuracy: 0.8916 - val_loss: 0.3349 - val_accuracy: 0.8770\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2873 - accuracy: 0.8941 - val_loss: 0.3301 - val_accuracy: 0.8812\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2807 - accuracy: 0.8966 - val_loss: 0.3285 - val_accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2754 - accuracy: 0.8990 - val_loss: 0.3257 - val_accuracy: 0.8794\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2702 - accuracy: 0.9009 - val_loss: 0.3280 - val_accuracy: 0.8838\n"
     ]
    }
   ],
   "source": [
    "# Keras supports learning rate scheduling by extending the LearningRateSchedule class, and then passing that to a optimizer.  To implement a PowerSchedule, we can do the following:\n",
    "\n",
    "class PowerScheduling(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_rate, step_param):\n",
    "        super(PowerScheduling, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.step_param = step_param\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Note the \"cast\" here - necessary to make sure we get the correct type back\n",
    "        return self.initial_learning_rate / (1 + tf.cast(step, tf.float32) / self.step_param) ** self.decay_rate\n",
    "\n",
    "\n",
    "# Example usage\n",
    "initial_learning_rate = 0.1\n",
    "decay_rate = 1.0  # For linear decay\n",
    "step_param = 1000.0  # Determines how quickly the learning rate decreases\n",
    "\n",
    "lr_schedule = PowerScheduling(initial_learning_rate, decay_rate, step_param)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "history_power_scheduling = build_and_train_model(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Exponential Scheduling:**\n",
    "   - **Description:** Cuts the learning rate by a factor of $ \\gamma $ every $ s $ steps.\n",
    "   - **Formulation:** $ \\eta(t) = \\eta_0 \\cdot \\gamma^{t/s} $.\n",
    "   - **Senior et al's Recommendation:** Particularly recommended for its effectiveness and simplicity in many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer name SGD\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5548 - accuracy: 0.7962 - val_loss: 0.4356 - val_accuracy: 0.8314\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4015 - accuracy: 0.8519 - val_loss: 0.4274 - val_accuracy: 0.8342\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3577 - accuracy: 0.8673 - val_loss: 0.3823 - val_accuracy: 0.8600\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3333 - accuracy: 0.8764 - val_loss: 0.3678 - val_accuracy: 0.8694\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3159 - accuracy: 0.8821 - val_loss: 0.3414 - val_accuracy: 0.8704\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2994 - accuracy: 0.8884 - val_loss: 0.3738 - val_accuracy: 0.8694\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2887 - accuracy: 0.8904 - val_loss: 0.3322 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2790 - accuracy: 0.8942 - val_loss: 0.3563 - val_accuracy: 0.8698\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2687 - accuracy: 0.8994 - val_loss: 0.3305 - val_accuracy: 0.8826\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2595 - accuracy: 0.9014 - val_loss: 0.3437 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# First, we'll calculate a learning rate based on the number of batches we anticipate\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
    "\n",
    "lr0=.1\n",
    "\n",
    "# Keras now has an ExponentialDecay scheduling class we can use here\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr0,\n",
    "                                                             decay_steps=n_steps,\n",
    "                                                             decay_rate=.95)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "history_exponential_scheduling = build_and_train_model(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Piecewise Constant Scheduling:**\n",
    "   - **Description:** Uses a constant learning rate for a set number of epochs and then lowers it to another constant rate for another set of epochs, and so on.\n",
    "   - **Formulation:** Implementing a pre-defined series of constant learning rates at different training stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer name SGD\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.5547 - accuracy: 0.7952 - val_loss: 0.4373 - val_accuracy: 0.8314\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4011 - accuracy: 0.8524 - val_loss: 0.4112 - val_accuracy: 0.8392\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3586 - accuracy: 0.8663 - val_loss: 0.4001 - val_accuracy: 0.8560\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3060 - accuracy: 0.8870 - val_loss: 0.3397 - val_accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2924 - accuracy: 0.8913 - val_loss: 0.3216 - val_accuracy: 0.8790\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2800 - accuracy: 0.8951 - val_loss: 0.3353 - val_accuracy: 0.8788\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2471 - accuracy: 0.9077 - val_loss: 0.3094 - val_accuracy: 0.8852\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2420 - accuracy: 0.9102 - val_loss: 0.3080 - val_accuracy: 0.8862\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2396 - accuracy: 0.9112 - val_loss: 0.3078 - val_accuracy: 0.8846\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2375 - accuracy: 0.9111 - val_loss: 0.3077 - val_accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want a learning rate of .1 for the first 3 epochs, .05 for the next three, and then .005 for the remainder.  So, we'll calculate the number of batches per epoch given our data.\n",
    "epoch_steps = math.ceil(len(X_train) / batch_size)\n",
    "b1 = epoch_steps * 3\n",
    "b2 = b1+epoch_steps * 3\n",
    "\n",
    "\n",
    "boundaries = [b1, b2]\n",
    "values = [.1, 0.05, 0.005]\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "history_piecewise = build_and_train_model(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Performance Scheduling:**\n",
    "   - **Description:** Measures the validation error every $ N $ steps (or epochs) and reduces the learning rate by $ \\lambda $ if the error has stopped dropping.\n",
    "   - **Senior et al's Recommendation:** Advised for its adaptability to the model’s actual progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.6730 - accuracy: 0.7705 - val_loss: 0.4748 - val_accuracy: 0.8334 - lr: 0.0100\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4679 - accuracy: 0.8339 - val_loss: 0.4465 - val_accuracy: 0.8354 - lr: 0.0100\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4212 - accuracy: 0.8478 - val_loss: 0.4043 - val_accuracy: 0.8562 - lr: 0.0100\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3967 - accuracy: 0.8595 - val_loss: 0.3792 - val_accuracy: 0.8622 - lr: 0.0100\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3766 - accuracy: 0.8663 - val_loss: 0.3777 - val_accuracy: 0.8648 - lr: 0.0100\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3607 - accuracy: 0.8718 - val_loss: 0.3845 - val_accuracy: 0.8642 - lr: 0.0100\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3485 - accuracy: 0.8741 - val_loss: 0.3617 - val_accuracy: 0.8680 - lr: 0.0100\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3367 - accuracy: 0.8785 - val_loss: 0.3544 - val_accuracy: 0.8678 - lr: 0.0100\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3273 - accuracy: 0.8808 - val_loss: 0.3432 - val_accuracy: 0.8720 - lr: 0.0100\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3169 - accuracy: 0.8851 - val_loss: 0.3469 - val_accuracy: 0.8698 - lr: 0.0100\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3099 - accuracy: 0.8883 - val_loss: 0.3560 - val_accuracy: 0.8666 - lr: 0.0100\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2903 - accuracy: 0.8946 - val_loss: 0.3340 - val_accuracy: 0.8770 - lr: 0.0050\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2859 - accuracy: 0.8973 - val_loss: 0.3187 - val_accuracy: 0.8794 - lr: 0.0050\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2817 - accuracy: 0.8978 - val_loss: 0.3269 - val_accuracy: 0.8800 - lr: 0.0050\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2785 - accuracy: 0.8988 - val_loss: 0.3260 - val_accuracy: 0.8806 - lr: 0.0050\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2684 - accuracy: 0.9031 - val_loss: 0.3153 - val_accuracy: 0.8800 - lr: 0.0025\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2665 - accuracy: 0.9031 - val_loss: 0.3175 - val_accuracy: 0.8800 - lr: 0.0025\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2643 - accuracy: 0.9037 - val_loss: 0.3194 - val_accuracy: 0.8808 - lr: 0.0025\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2597 - accuracy: 0.9055 - val_loss: 0.3130 - val_accuracy: 0.8836 - lr: 0.0012\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2585 - accuracy: 0.9060 - val_loss: 0.3153 - val_accuracy: 0.8818 - lr: 0.0012\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2576 - accuracy: 0.9067 - val_loss: 0.3137 - val_accuracy: 0.8824 - lr: 0.0012\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2550 - accuracy: 0.9085 - val_loss: 0.3143 - val_accuracy: 0.8834 - lr: 6.2500e-04\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2545 - accuracy: 0.9077 - val_loss: 0.3133 - val_accuracy: 0.8830 - lr: 6.2500e-04\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2531 - accuracy: 0.9089 - val_loss: 0.3136 - val_accuracy: 0.8812 - lr: 3.1250e-04\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2529 - accuracy: 0.9091 - val_loss: 0.3137 - val_accuracy: 0.8820 - lr: 3.1250e-04\n"
     ]
    }
   ],
   "source": [
    "# Performance based scheduling requires the use of a callback, so we can evaluate the performance of the network\n",
    "# We can do it like this\n",
    "lr0 = .01\n",
    "model = build_model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "history_performance_scheduling = model.fit(X_train, y_train, epochs=25,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_performance_scheduling.epoch, history_performance_scheduling.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, 24)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history_performance_scheduling.epoch, history_performance_scheduling.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. **1cycle Scheduling:**\n",
    "   - **Description:** Briefly increases the learning rate halfway through training before decreasing it. Proposed by [Leslie N. Smith in 2018](https://arxiv.org/abs/1803.09820).\n",
    "   - **Benefits (Smith, 2018):** Shown to lead to faster convergence and better performance. It allows the model to explore a wider range of parameters due to the higher learning rate in the middle of training.\n",
    "   - **Formulation:** Starts with a low learning rate, increases it linearly for the first half of training, and then decreases it linearly for the second half, possibly followed by a few epochs with a very small learning rate to finalize training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the 1-cycle learning rate scheduler in Keras is not hard, but it requires us to obtain an optimal learning rate first before creating a scheduler.  We did this above.  Here, I'm implementing the LR scheduler as a callback due to some of the complexities of working with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
    "                 last_iterations=None, last_lr=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
    "                                   self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                   self.max_lr, self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                   self.start_lr, self.last_lr)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 2s 3ms/step - loss: 0.9710 - accuracy: 0.6851 - val_loss: 0.6070 - val_accuracy: 0.7960\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.5728 - accuracy: 0.8006 - val_loss: 0.5020 - val_accuracy: 0.8242\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.5022 - accuracy: 0.8226 - val_loss: 0.4686 - val_accuracy: 0.8290\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4572 - accuracy: 0.8395 - val_loss: 0.4385 - val_accuracy: 0.8408\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4292 - accuracy: 0.8481 - val_loss: 0.4149 - val_accuracy: 0.8460\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4028 - accuracy: 0.8546 - val_loss: 0.4988 - val_accuracy: 0.8170\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3865 - accuracy: 0.8603 - val_loss: 0.4024 - val_accuracy: 0.8546\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3704 - accuracy: 0.8656 - val_loss: 0.3918 - val_accuracy: 0.8582\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3584 - accuracy: 0.8699 - val_loss: 0.3710 - val_accuracy: 0.8720\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3435 - accuracy: 0.8745 - val_loss: 0.5405 - val_accuracy: 0.8226\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8777 - val_loss: 0.4275 - val_accuracy: 0.8382\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8801 - val_loss: 0.3624 - val_accuracy: 0.8638\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3021 - accuracy: 0.8890 - val_loss: 0.3286 - val_accuracy: 0.8772\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.8947 - val_loss: 0.3952 - val_accuracy: 0.8600\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2726 - accuracy: 0.8996 - val_loss: 0.3385 - val_accuracy: 0.8804\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2627 - accuracy: 0.9033 - val_loss: 0.3344 - val_accuracy: 0.8790\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2525 - accuracy: 0.9059 - val_loss: 0.3235 - val_accuracy: 0.8798\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2417 - accuracy: 0.9105 - val_loss: 0.3208 - val_accuracy: 0.8854\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2337 - accuracy: 0.9140 - val_loss: 0.3180 - val_accuracy: 0.8826\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2259 - accuracy: 0.9169 - val_loss: 0.3242 - val_accuracy: 0.8850\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2187 - accuracy: 0.9186 - val_loss: 0.3087 - val_accuracy: 0.8888\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2119 - accuracy: 0.9226 - val_loss: 0.3122 - val_accuracy: 0.8884\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2067 - accuracy: 0.9243 - val_loss: 0.3100 - val_accuracy: 0.8902\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2036 - accuracy: 0.9252 - val_loss: 0.3108 - val_accuracy: 0.8900\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2013 - accuracy: 0.9266 - val_loss: 0.3090 - val_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs,\n",
    "                             max_lr=0.1)\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Learning rate scheduling is a powerful tool in the training of neural networks. Different scheduling methods offer various advantages, and the choice of method can depend on the specific characteristics of the task, the dataset, and the desired training dynamics. While 1cycle scheduling has shown promising results for faster convergence, the recommendations of Senior et al. for performance or exponential scheduling highlight the importance of choosing a method that aligns with the model’s learning progress and the practitioner’s familiarity with the training dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
